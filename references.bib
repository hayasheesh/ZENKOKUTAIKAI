@techReport{G202023,
   author = {G20},
   city = {New Delhi, India},
   institution = {G20},
   month = {9},
   title = {G20 New Delhi Leaders’ Declaration},
   year = {2023}
}
@techReport{InternationalRenewableEnergyAgency2025,
   author = {International Renewable Energy Agency},
   city = {Abu Dhabi, United Arab Emirates},
   institution = {IRENA},
   month = {3},
   title = {Renewable Capacity Highlights 2025},
   year = {2025}
}
@article{Lund2015,
   author = {Peter D. Lund and Juuso Lindgren and Jani Mikkola and Jyri Salpakari},
   doi = {10.1016/j.rser.2015.01.057},
   issn = {13640321},
   journal = {Renewable and Sustainable Energy Reviews},
   month = {5},
   pages = {785-807},
   title = {Review of energy system flexibility measures to enable high levels of variable renewable electricity},
   volume = {45},
   year = {2015}
}
@inproceedings{Inala2019,
   author = {Krishna Pavan Inala and Sanjay Kumar Bose and Praveen Kumar},
   doi = {10.1109/INDICON47234.2019.9028899},
   isbn = {978-1-7281-2327-1},
   booktitle = {2019 IEEE 16th India Council International Conference (INDICON)},
   month = {12},
   pages = {1-4},
   publisher = {IEEE},
   title = {Impact of Communication Network on V2G System in a Smart Grid Scenario},
   year = {2019}
}
@article{Chen2022,
   abstract = {<p>In order to further evaluate the impact of vehicle-to-grid (V2G) on the distribution network, this paper studies a method to assess the influence of electric vehicles participating in charge and discharge on the voltage quality of the distribution network. First, considering the state of charge of the EV, the participation of the owner and other factors, the charging and discharging model is built. Then, the probabilistic power flow calculation based on Latin hypercube sampling is used to obtain the probability distribution of the voltage amplitude of the charge and discharge load connected to the distribution network, and finally the evaluation index is established to quantify and calculate the voltage quality of the distribution network participating in the V2G process of electric vehicles. Simulation results show that the evaluation method has the advantage of fast calculation speed while ensuring known accuracy, introduces the probability distribution of expected value and variance quantification of voltage amplitude, more intuitively understands the degree of influence on voltage quality before and after V2G, and can effectively assess the impact of electric vehicles accessing the distribution network in V2G mode on the power quality of low-voltage residential areas and industrial and commercial areas, and this evaluation method can provide useful reference for the formulation of future V2G control strategies and the planning of future urban power grids.</p>},
   author = {Wei Chen and Lei Zheng and Hengjie Li and Xiping Pei},
   doi = {10.3390/en15114170},
   issn = {1996-1073},
   issue = {11},
   journal = {Energies},
   month = {6},
   pages = {4170},
   title = {An Assessment Method for the Impact of Electric Vehicle Participation in V2G on the Voltage Quality of the Distribution Network},
   volume = {15},
   year = {2022}
}
@article{Richardson2012,
   author = {Peter Richardson and Damian Flynn and Andrew Keane},
   doi = {10.1109/TPWRS.2011.2158247},
   issn = {0885-8950},
   issue = {1},
   journal = {IEEE Transactions on Power Systems},
   month = {2},
   pages = {268-279},
   title = {Optimal Charging of Electric Vehicles in Low-Voltage Distribution Systems},
   volume = {27},
   year = {2012}
}
@article{Lillicrap2019,
   abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
   author = {Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
   month = {7},
   title = {Continuous control with deep reinforcement learning},
   year = {2019}
}
@article{Lowe2020,
   abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
   author = {Ryan Lowe and Yi Wu and Aviv Tamar and Jean Harb and Pieter Abbeel and Igor Mordatch},
   month = {3},
   title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
   year = {2020}
}
@article{Rashid2018,
   abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods.},
   author = {Tabish Rashid and Mikayel Samvelyan and Christian Schroeder de Witt and Gregory Farquhar and Jakob Foerster and Shimon Whiteson},
   month = {6},
   title = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
   year = {2018}
}
@article{Foerster2018,
   abstract = {<p>Many real-world problems, such as network packet routing and the coordination of autonomous vehicles, are naturally modelled as cooperative multi-agent systems. There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems. To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients. COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies. In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed. COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass. We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability. COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.</p>},
   author = {Jakob Foerster and Gregory Farquhar and Triantafyllos Afouras and Nantas Nardelli and Shimon Whiteson},
   doi = {10.1609/aaai.v32i1.11794},
   issn = {2374-3468},
   issue = {1},
   journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
   month = {4},
   title = {Counterfactual Multi-Agent Policy Gradients},
   volume = {32},
   year = {2018}
}
@article{Zadaianchuk2020,
   abstract = {Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model. We show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects.},
   author = {Andrii Zadaianchuk and Maximilian Seitzer and Georg Martius},
   month = {11},
   title = {Self-supervised Visual Reinforcement Learning with Object-centric Representations},
   year = {2020}
}
@article{Sheikh2020,
   abstract = {Many cooperative multi-agent problems require agents to learn individual tasks while contributing to the collective success of the group. This is a challenging task for current state-of-the-art multi-agent reinforcement algorithms that are designed to either maximize the global reward of the team or the individual local rewards. The problem is exacerbated when either of the rewards is sparse leading to unstable learning. To address this problem, we present Decomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG): a novel cooperative multi-agent reinforcement learning framework that simultaneously learns to maximize the global and local rewards. We evaluate our solution on the challenging defensive escort team problem and show that our solution achieves a significantly better and more stable performance than the direct adaptation of the MADDPG algorithm.},
   author = {Hassam Ullah Sheikh and Ladislau Bölöni},
   month = {3},
   title = {Multi-Agent Reinforcement Learning for Problems with Combined Individual and Team Reward},
   year = {2020}
}
@article{Vaswani2023,
   abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
   author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
   month = {8},
   title = {Attention Is All You Need},
   year = {2023}
}
@article{Mayne2000,
   author = {D.Q. Mayne and J.B. Rawlings and C.V. Rao and P.O.M. Scokaert},
   doi = {10.1016/S0005-1098(99)00214-9},
   issn = {00051098},
   issue = {6},
   journal = {Automatica},
   month = {6},
   pages = {789-814},
   title = {Constrained model predictive control: Stability and optimality},
   volume = {36},
   year = {2000}
}
@article{Imanaka2025,
   abstract = {<p> The flexibility of distributed energy resources (DERs) has been receiving increasing attention. One important issue to use DERs' flexibility for fast demand response, like for secondary reserves, is their full activation time (FAT) and ICT (Information communication technology) response time (IRT). Though studies have reported the inherent activation time of various DERs, time‐synchronized measurement results of both ICT latency and the FAT are still lacking. We have constructed an experimental system to measure them synchronously with the local time server, excluding cloud timestamps. This paper reports on the experimental system and the characteristics of measured FATs and IRTs of the following DERs; one vehicle‐to‐home equipped charger (controlled both via the Internet and via the LTE‐M) and one Mode 3 normal charger, as examples. This information of FAT will be useful for designing the group control and monitoring system of the electric vehicle charging for fast demand response. © 2025 The Author(s). <italic>IEEJ Transactions on Electrical and Electronic Engineering</italic> published by Institute of Electrical Engineers of Japan and Wiley Periodicals LLC. </p>},
   author = {Masaki Imanaka and Hiroyuki Baba},
   doi = {10.1002/tee.70176},
   issn = {1931-4973},
   journal = {IEEJ Transactions on Electrical and Electronic Engineering},
   month = {9},
   title = {Measurement of both full activation time and ICT response time for fast demand response of electric-vehicle charging},
   year = {2025}
}
@misc{CaliforniaIndependentSystemOperatorCAISO2024,
   author = {California Independent System Operator (CAISO)},
   journal = {CAISO News Release},
   month = {7},
   title = {WEIM entities share sizeable energy transfers during Q2 2024},
   year = {2024}
}
@misc{Walton2015,
   author = {Robert Walton},
   journal = {Utility Dive},
   month = {2},
   title = {How the West’s new energy imbalance market is building a smarter energy system},
   year = {2015}
}
@techReport{JapanElectricPowerExchangeJEPX2025,
   author = {Japan Electric Power Exchange (JEPX)},
   city = {Tokyo, Japan},
   institution = {Japan Electric Power Exchange (JEPX)},
   month = {5},
   title = {Supply and Demand Adjustment Market Product Guide Ver.5},
   year = {2025}
}
@techReport{EuropeanCommission2017,
   author = {European Commission},
   city = {Brussels},
   institution = {European Commission},
   month = {11},
   title = {Commission Regulation (EU) 2017/2195 of 23 November 2017 establishing a guideline on electricity balancing},
   year = {2017}
}
@techReport{MinistryofEconomy2019,
   author = {Trade and Industry (METI) Ministry of Economy},
   city = {Tokyo, Japan},
   institution = {Ministry of Economy, Trade and Industry (METI)},
   month = {7},
   title = {Second interim report of the 30th meeting of the Working Group on System Review, Electricity and Gas Basic Policy Subcommittee [11.1]},
   year = {2019}
}
@techReport{NewYorkIndependentSystemOperatorNYISO2005,
   author = {New York Independent System Operator (NYISO)},
   city = {Albany, NY, USA},
   institution = {New York Independent System Operator (NYISO)},
   month = {10},
   title = {Ancillary Services Manual},
   year = {2005}
}
@misc{OpenEMSAssociation2025,
   author = {OpenEMS Association},
   journal = {OpenEMS Documentation},
   month = {12},
   title = {OpenEMS Edge: Controllers},
   year = {2025}
}
@techReport{ElaadNLWorkplace,
   author = {ElaadNL},
   city = {The Netherlands},
   institution = {ElaadNL},
   title = {Dataset 2: Distribution of arrival times on weekdays (workplace)}
}
@techReport{DistributedElectricalSystemsLaboratoryDESL2025,
   author = {EPFL Distributed Electrical Systems Laboratory (DESL)},
   city = {Lausanne, Switzerland},
   institution = {Distributed Electrical Systems Laboratory (DESL), EPFL},
   month = {6},
   title = {Level-3 EV charging dataset},
   year = {2025}
}
@techReport{CaliforniaInstituteofTechnology2019,
   author = {California Institute of Technology},
   city = {Pasadena, CA, USA},
   institution = {California Institute of Technology},
   title = {ACN-Data: EV charging dataset (JPL site)},
   year = {2019}
}
@techReport{PJMInterconnection2024,
   author = {L.L.C. PJM Interconnection},
   institution = {PJM Interconnection, L.L.C.},
   month = {10},
   title = {RTO regulation signal data},
   year = {2024}
}
@article{Liu2016,
   author = {Guodong Liu and Yan Xu and Kevin Tomsovic},
   doi = {10.1109/TSG.2015.2476669},
   issn = {1949-3053},
   issue = {1},
   journal = {IEEE Transactions on Smart Grid},
   month = {1},
   pages = {227-237},
   title = {Bidding Strategy for Microgrid in Day-Ahead Market Based on Hybrid Stochastic/Robust Optimization},
   volume = {7},
   year = {2016}
}
@article{Tajeddini2014,
   author = {Mohammad Amin Tajeddini and Ashkan Rahimi-Kian and Alireza Soroudi},
   doi = {10.1016/j.energy.2014.06.110},
   issn = {03605442},
   journal = {Energy},
   month = {8},
   pages = {958-967},
   title = {Risk averse optimal operation of a virtual power plant using two stage stochastic programming},
   volume = {73},
   year = {2014}
}
@article{Shayegan-Rad2017,
   author = {Ali Shayegan-Rad and Ali Badri and Ali Zangeneh},
   doi = {10.1016/j.energy.2017.01.006},
   issn = {03605442},
   journal = {Energy},
   month = {2},
   pages = {114-125},
   title = {Day-ahead scheduling of virtual power plant in joint energy and regulation reserve markets under uncertainties},
   volume = {121},
   year = {2017}
}
@article{Wangsupphaphol2022,
   abstract = {<p>Government policies are crucial factors for supporting the growth of the electric vehicle (EV) industry—a growth that can be encouraged, for example, by subsidization designed to reduce the considerable anxiety stemming from the inconvenience of refueling at public charging stations. Subsidizing low priority charging for residential enables cost-effective load management for example controlling of EV charging power for grid reliability at the off-peak rate for 24 h. This solution provides the convenient recharging of EVs at home and prevents an expensive grid upgradation. To advance our understanding of the EV situation, this research used a regression model to forecast the growth rate of the EV market alongside the EV expansion policies in Thailand. The agreement between a policy and forecasting urges the government to prepare power system adequacy for EV loading. The analysis showed that power demand and voltage reduction in a typical low-voltage distribution system that assumes maximum EV loading constitute voltage violations. To address this limitation, this study proposed a rule-based strategy wherein low priority smart EV charging is regulated. The numerical validation of the strategy indicated that the strategy reduced power demand by 25% and 39% compared with that achieved under uncontrolled and time of use (TOU) charging, respectively. The strategy also limited voltage reduction and prolonged battery life. The study presents implications for policymakers and electricity companies with respect to possible technical approaches to stimulating EV penetration.</p>},
   author = {Aree Wangsupphaphol and Surachai Chaitusaney},
   doi = {10.3390/su14106053},
   issn = {2071-1050},
   issue = {10},
   journal = {Sustainability},
   month = {5},
   pages = {6053},
   title = {Subsidizing Residential Low Priority Smart Charging: A Power Management Strategy for Electric Vehicle in Thailand},
   volume = {14},
   year = {2022}
}
@article{ElHarouri2023,
   abstract = {<p>Electric vehicles (EV) and photovoltaic (PV) systems are increasingly becoming environmentally friendly and more affordable solutions for consumers. This article discusses the integration of PV and EV in a residential system to meet the requirements of residential loads taking into account the PV supplied power, availability and the state of charge (SOC) of EVs. A hybrid control model has been proposed to control the residential system. The combined PI-Fuzzy logic controller is employed to control the buck-boost bi-directional converter. The DC-AC grid-side converter is controlled by the ADRC controller. The effectiveness of PI-Fuzzy logic controller in reducing voltage and current ripples and ADRC controller in rejecting disturbances is demonstrated in each case. A rule-based energy management strategy has been proposed to control the flow of energy between the components of the residential system. The suggested energy management system (EMS) covers every scenario that might occur. Whether the EV is linked to the home or not, and also takes into account the owner using the EV in an emergency situation. The EV operates in two modes, Home-to-Vehicle (H2V) mode and Vehicle-to-Home (V2H) mode, depending on the power produced by the PV and the conditions related to the EV. All possible scenarios are tested and validated. The simulation results show that the proposed EMS is a reliable solution that can reduce the power grid intervention.</p>},
   author = {Khadija El Harouri and Soumia El Hani and Nisrine Naseri and Elhoussin Elbouchikhi and Mohamed Benbouzid and Sondes Skander-Mustapha},
   doi = {10.3390/designs7020052},
   issn = {2411-9660},
   issue = {2},
   journal = {Designs},
   month = {4},
   pages = {52},
   title = {Hybrid Control and Energy Management of a Residential System Integrating Vehicle-to-Home Technology},
   volume = {7},
   year = {2023}
}
@article{Shohan2024,
   abstract = {<p>As the adoption of electric vehicles (EVs) continues to rise, efficient scheduling methods that minimize operational costs are critical. This paper introduces a novel EV scheduling method utilizing a heuristic graph-search algorithm for cost minimization due to its admissible nature. The approach optimizes EV charging and discharging schedules by considering real-time energy prices and battery degradation costs. The method is tested on systems with solar generation, electric loads, and EVs featuring vehicle-to-grid (V2G) connections. Various charging rates, such as standard, fast, and supercharging, along with uncertainties in EV arrival and departure times, are factored into the analysis. Results from various case studies demonstrate that the proposed method outperforms popular heuristic optimization techniques, such as particle swarm optimization and genetic algorithms, by 3–5% for different real-time energy prices. Additionally, the method’s effectiveness in reducing operational costs for workplace EVs is confirmed through extensive case studies under varying uncertain conditions. Finally, the system is implemented on a digital real-time simulator with DNP3 communication, where real-time results align closely with offline simulations, confirming the algorithm’s efficacy for real-world applications.</p>},
   author = {Md Jamal Ahmed Shohan and Md Maidul Islam and Sophia Owais and Md Omar Faruque},
   doi = {10.3390/en17215278},
   issn = {1996-1073},
   issue = {21},
   journal = {Energies},
   month = {10},
   pages = {5278},
   title = {Optimal Energy Management of EVs at Workplaces and Residential Buildings Using Heuristic Graph-Search Algorithm},
   volume = {17},
   year = {2024}
}
@article{Liu2023,
   abstract = {<p>The rapid development of electric vehicle (EV) technology and the consequent charging demand have brought challenges to the stable operation of distribution networks (DNs). The problem of the collaborative optimization of the charging scheduling of EVs and voltage control of the DN is intractable because the uncertainties of both EVs and the DN need to be considered. In this paper, we propose a deep reinforcement learning (DRL) approach to coordinate EV charging scheduling and distribution network voltage control. The DRL-based strategy contains two layers, the upper layer aims to reduce the operating costs of power generation of distributed generators and power consumption of EVs, and the lower layer controls the Volt/Var devices to maintain the voltage stability of the distribution network. We model the coordinate EV charging scheduling and voltage control problem in the distribution network as a Markov decision process (MDP). The model considers uncertainties of charging process caused by the charging behavior of EV users, as well as the uncertainty of uncontrollable load, system dynamic electricity price and renewable energy generation. Since the model has a dynamic state space and mixed action outputs, a framework of deep deterministic policy gradient (DDPG) is adopted to train the two-layer agent and the policy network is designed to output discrete and continuous control actions. Simulation and numerical results on the IEEE-33 bus test system demonstrate the effectiveness of the proposed method in collaborative EV charging scheduling and distribution network voltage stabilization.</p>},
   author = {Ding Liu and Peng Zeng and Shijie Cui and Chunhe Song},
   doi = {10.3390/s23031618},
   issn = {1424-8220},
   issue = {3},
   journal = {Sensors},
   month = {2},
   pages = {1618},
   title = {Deep Reinforcement Learning for Charging Scheduling of Electric Vehicles Considering Distribution Network Voltage Stability},
   volume = {23},
   year = {2023}
}
@article{Hao2025,
   abstract = {<p>Electric vehicles (EVs) with managed charging and discharging schedules have the potential to reduce costs, enhance grid resilience, and facilitate integration of renewable energy sources. However, the heterogeneity of consumer travel patterns and the variability of renewable energy generation present significant challenges to existing control strategies, often resulting in issues such as the “curse of dimensionality.” This study proposes a mobility-aware deep reinforcement learning-based charging control strategy using the Deep Q-Network (DQN) algorithm to minimize charging costs and maximize photovoltaic (PV) energy utilization. Leveraging real-time electricity prices, real-world EV travel data, and actual PV generation profiles, the proposed framework achieves low charging costs, high solar energy utilization, and reduced carbon emissions—approaching the performance of an ideal offline optimization algorithm with perfect foresight, and substantially outperforming baseline strategies such as random charging, Charge-As-Soon-As-Possible (CASAP), and greedy charging. Specifically, the RL-based approach reduces charging costs by 55% and lowers carbon emission by 11.6% compared to random charging, and achieves a PV utilization rate of 95%. Furthermore, the value of information regarding EV’s travel time and the building’s electricity demand is 2.4CNY/vehicle/day and $0.7/vehicle/day, respectively, underscoring the importance of addressing uncertainty in EV charging management. These findings demonstrate the feasibility and effectiveness of reinforcement learning in optimizing EV operations within integrated vehicle-grid-building-PV systems.</p>},
   author = {Xu Hao and Pengju Liu and Hongyu Pu and Fuda Gong and Fan Tong and Qi Chen and Lishuo Liu and Xiaoru Chen},
   doi = {10.1007/s43979-025-00142-x},
   issn = {2788-8614},
   issue = {1},
   journal = {Carbon Neutrality},
   month = {12},
   pages = {26},
   title = {Mobility-aware EV charging and discharging management in V2B-PV systems: a reinforcement learning framework},
   volume = {4},
   year = {2025}
}
@article{Azzouz2023,
   abstract = {<p>The worldwide adoption of Electric Vehicles (EVs) has embraced promising advancements toward a sustainable transportation system. However, the effective charging scheduling of EVs is not a trivial task due to the increase in the load demand in the Charging Stations (CSs) and the fluctuation of electricity prices. Moreover, other issues that raise concern among EV drivers are the long waiting time and the inability to charge the battery to the desired State of Charge (SOC). In order to alleviate the range of anxiety of users, we perform a Deep Reinforcement Learning (DRL) approach that provides the optimal charging time slots for EV based on the Photovoltaic power prices, the current EV SOC, the charging connector type, and the history of load demand profiles collected in different locations. Our implemented approach maximizes the EV profit while giving a margin of liberty to the EV drivers to select the preferred CS and the best charging time (i.e., morning, afternoon, evening, or night). The results analysis proves the effectiveness of the DRL model in minimizing the charging costs of the EV up to 60%, providing a full charging experience to the EV with a lower waiting time of less than or equal to 30 min.</p>},
   author = {Imen Azzouz and Wiem Fekih Hassen},
   doi = {10.3390/en16248102},
   issn = {1996-1073},
   issue = {24},
   journal = {Energies},
   month = {12},
   pages = {8102},
   title = {Optimization of Electric Vehicles Charging Scheduling Based on Deep Reinforcement Learning: A Decentralized Approach},
   volume = {16},
   year = {2023}
}
@article{Fernndez-Guillamn2020,
   author = {Ana Fernández-Guillamón and José Ignacio Sarasúa and Manuel Chazarra and Antonio Vigueras-Rodríguez and Daniel Fernández-Muñoz and Ángel Molina-García},
   doi = {10.1016/j.ijepes.2020.106044},
   issn = {01420615},
   journal = {International Journal of Electrical Power \& Energy Systems},
   month = {10},
   pages = {106044},
   title = {Frequency control analysis based on unit commitment schemes with high wind power integration: A Spanish isolated power system case study},
   volume = {121},
   year = {2020}
}
@article{Hu2021,
   author = {Qian Hu and Ziqing Zhu and Siqi Bu and Ka Wing Chan and Fangxing Li},
   doi = {10.1016/j.apenergy.2021.116938},
   issn = {03062619},
   journal = {Applied Energy},
   month = {7},
   pages = {116938},
   title = {A multi-market nanogrid P2P energy and ancillary service trading paradigm: Mechanisms and implementations},
   volume = {293},
   year = {2021}
}
@article{Li2024,
   author = {Xiangyu Li and Fengji Luo and Chaojie Li},
   doi = {10.1016/j.apenergy.2024.122813},
   issn = {03062619},
   journal = {Applied Energy},
   month = {4},
   pages = {122813},
   title = {Multi-agent deep reinforcement learning-based autonomous decision-making framework for community virtual power plants},
   volume = {360},
   year = {2024}
}
@article{Shayegan-Rad2017,
   author = {Ali Shayegan-Rad and Ali Badri and Ali Zangeneh},
   doi = {10.1016/j.energy.2017.01.006},
   issn = {03605442},
   journal = {Energy},
   month = {2},
   pages = {114-125},
   title = {Day-ahead scheduling of virtual power plant in joint energy and regulation reserve markets under uncertainties},
   volume = {121},
   year = {2017}
}
@article{Cao2022,
   author = {Jinye Cao and Yingying Zheng and Xueru Han and Dechang Yang and Jianshu Yu and Nikita Tomin and Payman Dehghanian},
   doi = {10.1016/j.egyr.2022.05.255},
   issn = {23524847},
   journal = {Energy Reports},
   month = {11},
   pages = {7374-7385},
   title = {Two-stage optimization of a virtual power plant incorporating with demand response and energy complementation},
   volume = {8},
   year = {2022}
}
@article{Chen2024,
   author = {Qixin Chen and Ruike Lyu and Hongye Guo and Xiangbo Su},
   doi = {10.1016/j.apenergy.2024.122876},
   issn = {03062619},
   journal = {Applied Energy},
   month = {5},
   pages = {122876},
   title = {Real-time operation strategy of virtual power plants with optimal power disaggregation among heterogeneous resources},
   volume = {361},
   year = {2024}
}
@inbook{Konstantinidis2022,
   author = {George Konstantinidis and Emmanuel Karapidakis and Alexandros Paspatis},
   doi = {10.1007/978-3-031-07520-9_14},
   pages = {149-157},
   title = {A Rule-Based Method for Efficient Electric Vehicle Charging Scheduling at Parking Lots},
   year = {2022}
}
@article{Suh2022,
   abstract = {<p>As demand for electric vehicles (EV) grows, interest in vehicle‐to‐grid (V2G) is increasing. Although V2G provides additional flexibility for power systems, it can be an inconvenience for the EV owner and shorten the cycle life of EV batteries. In this paper, we propose a power‐imbalance‐based droop control (PIDC) for V2G, which uses estimated active power imbalance between supply and demand, without any communications or dispatch signal from the automatic generation control (AGC). Unlike the existing frequency droop control, the proposed method controls V2G based on the estimated imbalance. The Imbalance between electricity supply and demand was estimated using the swing equation, frequency deviation, and rate of change of frequency (RoCoF). Conventional frequency droop control has a slow response speed because it only considers the frequency deviation. Using the PIDC, the critical frequency drop was discerned faster and the V2G could react more quickly in critical frequency drop situations. Additionally, battery cycle life was preserved in ordinary frequency fluctuations where V2G is unnecessary. This study used MATLAB R2020a to show the effectiveness and performance of PIDC. The case study results showed that PIDC has a faster response speed and preserves EV battery cycle life compared to frequency droop control.</p>},
   author = {Jaewan Suh and Sungyoon Song and Gilsoo Jang},
   doi = {10.1049/gtd2.12528},
   issn = {1751-8687},
   issue = {17},
   journal = {IET Generation, Transmission \& Distribution},
   month = {9},
   pages = {3374-3383},
   title = {Power imbalance‐based droop control for vehicle to grid in primary frequency regulation},
   volume = {16},
   year = {2022}
}
@article{Guan2024,
   author = {Yuxiang Guan and Jin Zhang and Wenhao Ma and Liang Che},
   doi = {10.1016/j.ijepes.2024.109863},
   issn = {01420615},
   journal = {International Journal of Electrical Power \& Energy Systems},
   month = {6},
   pages = {109863},
   title = {Rule-based shields embedded safe reinforcement learning approach for electric vehicle charging control},
   volume = {157},
   year = {2024}
}
@article{Shojaeighadikolaei2024,
   abstract = {The widespread adoption of electric vehicles (EVs) poses several challenges to power distribution networks and smart grid infrastructure due to the possibility of significantly increasing electricity demands, especially during peak hours. Furthermore, when EVs participate in demand-side management programs, charging expenses can be reduced by using optimal charging control policies that fully utilize real-time pricing schemes. However, devising optimal charging methods and control strategies for EVs is challenging due to various stochastic and uncertain environmental factors. Currently, most EV charging controllers operate based on a centralized model. In this paper, we introduce a novel approach for distributed and cooperative charging strategy using a Multi-Agent Reinforcement Learning (MARL) framework. Our method is built upon the Deep Deterministic Policy Gradient (DDPG) algorithm for a group of EVs in a residential community, where all EVs are connected to a shared transformer. This method, referred to as CTDE-DDPG, adopts a Centralized Training Decentralized Execution (CTDE) approach to establish cooperation between agents during the training phase, while ensuring a distributed and privacy-preserving operation during execution. We theoretically examine the performance of centralized and decentralized critics for the DDPG-based MARL implementation and demonstrate their trade-offs. Furthermore, we numerically explore the efficiency, scalability, and performance of centralized and decentralized critics. Our theoretical and numerical results indicate that, despite higher policy gradient variances and training complexity, the CTDE-DDPG framework significantly improves charging efficiency by reducing total variation by approximately %36 and charging cost by around %9.1 on average...},
   author = {Amin Shojaeighadikolaei and Zsolt Talata and Morteza Hashemi},
   month = {4},
   title = {Centralized vs. Decentralized Multi-Agent Reinforcement Learning for Enhanced Control of Electric Vehicle Charging Networks},
   year = {2024}
}
@article{Zhang2024,
   author = {Bin Zhang and Di Cao and Weihao Hu and Amer M.Y.M. Ghias and Zhe Chen},
   doi = {10.1016/j.ijepes.2023.109641},
   issn = {01420615},
   journal = {International Journal of Electrical Power \& Energy Systems},
   month = {1},
   pages = {109641},
   title = {Physics-Informed Multi-Agent deep reinforcement learning enabled distributed voltage control for active distribution network using PV inverters},
   volume = {155},
   year = {2024}
}
@article{Yang2025,
   author = {Ting Yang and Zheming Xu and Shijie Ji and Guoliang Liu and Xinhong Li and Haibo Kong},
   doi = {10.1016/j.apenergy.2024.124641},
   issn = {03062619},
   journal = {Applied Energy},
   month = {1},
   pages = {124641},
   title = {Cooperative optimal dispatch of multi-microgrids for low carbon economy based on personalized federated reinforcement learning},
   volume = {378},
   year = {2025}
}
@article{Park2022,
   author = {Keonwoo Park and Ilkyeong Moon},
   doi = {10.1016/j.apenergy.2022.120111},
   issn = {03062619},
   journal = {Applied Energy},
   month = {12},
   pages = {120111},
   title = {Multi-agent deep reinforcement learning approach for EV charging scheduling in a smart grid},
   volume = {328},
   year = {2022}
}
@article{Li2024,
   author = {Xiangyu Li and Fengji Luo and Chaojie Li},
   doi = {10.1016/j.apenergy.2024.122813},
   issn = {03062619},
   journal = {Applied Energy},
   month = {4},
   pages = {122813},
   title = {Multi-agent deep reinforcement learning-based autonomous decision-making framework for community virtual power plants},
   volume = {360},
   year = {2024}
}
@article{Fan2022,
   abstract = {<p>With the development of micro gas turbines (MT) and power-to-gas (P2G) technology, the electric–gas system plays an important role in maintaining the stable, economical, and flexible operation of the microgrid. When subjected to power load disturbance and natural gas load disturbance, the system controller needs to coordinately control the frequency of the microgrid and the gas pressure at the natural gas pipeline nodes. Additionally, the reliability and stability of a multi-microgrid system are much higher than that of a single microgrid, but its control technology is more complicated. Thus, a frequency–pressure cooperative control strategy of a multi-microgrid oriented to an electric–gas system is proposed in this paper. Firstly, based on the analysis of the operating characteristics of the natural gas network and the coupling equipment, the dynamic model of natural gas transmission is built. Secondly, a multi-microgrid load frequency control model including MT, P2G equipment, electric vehicles (EVs), distributed power sources and loads has been established. In addition, according to the three control objectives of microgrid frequency, node pressure and system coordination and stability, the structure of a Muti-Agent Deep Deterministic Policy Gradient (MADDPG) controller is designed, then the definition of space and reward functions are completed. Finally, different cases are set up in the multi-microgrid, and the simulation results are compared with PI control and fuzzy control. The simulation results show that, the proposed MADDPG controller can greatly suppress the frequency deviation caused by wind power and load disturbances and the air pressure fluctuations caused by natural gas network load fluctuations. Additionally, it can coordinate well the overall stability between the sub-microgrids of multi-microgrid.</p>},
   author = {Peixiao Fan and Jia Hu and Song Ke and Yuxin Wen and Shaobo Yang and Jun Yang},
   doi = {10.3390/su14148886},
   issn = {2071-1050},
   issue = {14},
   journal = {Sustainability},
   month = {7},
   pages = {8886},
   title = {A Frequency–Pressure Cooperative Control Strategy of Multi-Microgrid with an Electric–Gas System Based on MADDPG},
   volume = {14},
   year = {2022}
}
@article{Liu2018,
   author = {Hui Liu and Junjian Qi and Jianhui Wang and Peijie Li and Canbing Li and Hua Wei},
   doi = {10.1109/TSG.2016.2641481},
   issn = {1949-3053},
   issue = {4},
   journal = {IEEE Transactions on Smart Grid},
   month = {7},
   pages = {3763-3772},
   title = {EV Dispatch Control for Supplementary Frequency Regulation Considering the Expectation of EV Owners},
   volume = {9},
   year = {2018}
}
@article{Alfaverh2023,
   author = {Fayiz Alfaverh and Mouloud Denaï and Yichuang Sun},
   doi = {10.1016/j.epsr.2022.108949},
   issn = {03787796},
   journal = {Electric Power Systems Research},
   month = {1},
   pages = {108949},
   title = {Optimal vehicle-to-grid control for supplementary frequency regulation using deep reinforcement learning},
   volume = {214},
   year = {2023}
}
@article{Zadaianchuk2020,
   abstract = {Autonomous agents need large repertoires of skills to act reasonably on new tasks that they have not seen before. However, acquiring these skills using only a stream of high-dimensional, unstructured, and unlabeled observations is a tricky challenge for any autonomous agent. Previous methods have used variational autoencoders to encode a scene into a low-dimensional vector that can be used as a goal for an agent to discover new skills. Nevertheless, in compositional/multi-object environments it is difficult to disentangle all the factors of variation into such a fixed-length representation of the whole scene. We propose to use object-centric representations as a modular and structured observation space, which is learned with a compositional generative world model. We show that the structure in the representations in combination with goal-conditioned attention policies helps the autonomous agent to discover and learn useful skills. These skills can be further combined to address compositional tasks like the manipulation of several different objects.},
   author = {Andrii Zadaianchuk and Maximilian Seitzer and Georg Martius},
   month = {11},
   title = {Self-supervised Visual Reinforcement Learning with Object-centric Representations},
   year = {2020}
}
@article{Christensen2025,
   abstract = {<p>The rapid electrification of transportation, driven by stringent decarbonization targets and supportive policies, poses significant challenges for distribution system operators (DSOs). When numerous electric vehicles (EVs) charge concurrently, local transformers risk overloading—a problem that current tariff-based strategies do not adequately address. This paper introduces an aggregator-based coordination mechanism that shifts EV charging from congested to underutilized periods using a rule-based scheduling algorithm. Unlike conventional methods that depend on complex real-time pricing signals or optimization-heavy solutions, the aggregator approach uses a simple yet effective “laxity” measure to prioritize charging flexibility. To assess technical and economic viability, a multi-agent simulation was developed to replicate residential user behavior and DSO constraints under the use of a 400 kVA low-voltage transformer. The results indicate that overloads are completely eliminated with minimal inconvenience to users, whose increased charging costs are offset by the aggregator at an annual total of under DKK 6000—significantly lower than the cost of infrastructure reinforcement. This study contributes by (i) quantifying the compensation needed to prevent large-scale overloads, (ii) presenting a replicable, computationally feasible, rule-based aggregator model for DSOs, and (iii) comparing aggregator solutions to costly transformer upgrades, underscoring the aggregator’s role as a viable tool for future distribution systems.</p>},
   author = {Kristoffer Christensen and Bo Nørregaard Jørgensen and Zheng Grace Ma},
   doi = {10.3390/su17093847},
   issn = {2071-1050},
   issue = {9},
   journal = {Sustainability},
   month = {4},
   pages = {3847},
   title = {A Multi-Agent, Laxity-Based Aggregation Strategy for Cost-Effective Electric Vehicle Charging and Local Transformer Overload Prevention},
   volume = {17},
   year = {2025}
}
