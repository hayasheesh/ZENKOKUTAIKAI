\documentclass[twocolumn,9pt,a4paper,dvipdfmx]{jsarticle}

% --- パッケージ設定 ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} % 英数字をTimes系に
\usepackage{float}               % [H]で図を強制配置
\usepackage[top=30mm, bottom=27mm, left=18mm, right=18mm, columnsep=7mm]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{secdot}
\usepackage{caption}
\usepackage{titlesec}
\usepackage{cite}
\usepackage{amsmath,amssymb}
\usepackage[none]{hyphenat} % 全体のハイフネーション抑止

% --- 図表と本文の間隔設定 ---
\setlength{\textfloatsep}{0pt} % ページ上下の図表と本文の間隔
\setlength{\intextsep}{0pt}    % 本文中の図表(h)と本文の間隔
\setlength{\floatsep}{0pt}     % 図表同士の間隔

% --- キャプション設定 ---
\captionsetup[figure]{font=small, labelsep=space, name=図, skip=3pt}
\captionsetup[table]{font=small, labelsep=space, name=表, skip=3pt}

% --- セクション設定 ---
\sectiondot{section}
\sectiondot{subsection}

% セクション:
% サイズ10pt, 行送り20pt
\titleformat{\section}
  {\fontsize{10pt}{20pt}\selectfont\mcfamily}
  {\thesection}{1em}{}
% 上下の余白を0ptに設定
\titlespacing{\section}{0pt}{0pt}{0pt}

% サブセクション:
% サイズ9pt, 行送り14pt
\renewcommand{\thesubsection}{$<$\thesection$\cdot$\arabic{subsection}$>$}
\titleformat{\subsection}[runin]
  {\fontsize{9pt}{14pt}\selectfont\mcfamily}
  {\thesubsection}{1em}{}
% 上下の余白を0ptに設定
\titlespacing{\subsection}{0pt}{0pt}{1em}

% 参考文献形式 (1)
\renewcommand{\citeleft}{[}
\renewcommand{\citeright}{]}
\makeatletter
\renewcommand{\@biblabel}[1]{[#1]} % ← (#1) から [#1] に変更
\makeatother

% --- 文書情報 ---
\date{}

\begin{document}
\pagestyle{empty} % ページ番号を消す

% --- タイトル部分 ---
\twocolumn[
    \begin{center}
        \vspace*{10mm}

        \parbox{134mm}{
            \centering
            \fontsize{18pt}{28pt}\selectfont \mcfamily
            マルチエージェント強化学習によるEV群VPPの充放電協調制御
        }

        \vspace{18pt}

        {\fontsize{12pt}{18pt}\selectfont \mcfamily
        林 弘辰\textsuperscript{*} (筑波大学)、 \fontsize{12pt}{18pt}\selectfont \mcfamily
        Xue Sihui\textsuperscript{} (筑波大学)、 \fontsize{12pt}{18pt}\selectfont \mcfamily
        小平　大輔\textsuperscript{} (筑波大学)  }

        \vspace{14pt}

        {\fontsize{9pt}{14pt}\selectfont
        Coordinated Charging and Discharging Control of an EV-Based VPP Using Multi-Agent Reinforcement Learning \par
        Koshin Hayashi (University of Tsukuba) \par}

        \vspace{14pt}
    \end{center}
]
\thispagestyle{empty} % 1ページ目も確実にページ番号を消す

% --- 本文開始 ---
\fontsize{9pt}{14pt}\selectfont

\section{まえがき}
バーチャルパワープラント（VPP）の需給調整市場参入では、リソースアグリゲーターが系統運用者から与えられる指令信号に対して、リアルタイムかつ低遅延でリソースを制御し追従する能力が求められる場合がある。一方、電気自動車（EV）を多数束ねたVPPを集中最適化で制御する場合、個別EV状態の収集・通信およびオンライン計算がボトルネックとなり、応答レイテンシ制約下では運用が困難になり得る。

一方、Multi-Agent Deep Deterministic Policy Gradient (MADDPG) に代表されるMARL（Multi-Agent Reinforcement Learning）はCTDE（Centralized Training and Decentralized Execution）の性質を持ち、実行時に頻繁な全体通信や大規模最適化を要さずに協調方策を実装できる\cite{Lowe2020}。EV群制御にMARLを適用した先行研究として、ユーザ側QoS（充電費用や充電状態（State of Charge: SoC）等）に重点を置くもの\cite{Park2022}や、系統側の指標に重点を置くもの\cite{Fan2022}があるが、双方を同一のMARL枠組みで明示的に最適化・評価する設計は限定的である。

ここで、本稿の貢献は次の2点である。(1) 需給調整市場への参加を想定し、低遅延かつスケーラブルな階層型MADDPG制御を設計した。(2) 日本の需給調整市場を想定したシミュレーション環境にてケーススタディを行い、指令追従率および出発時SoC満足度の両方を評価指標として有効性を検討する。

\section{提案手法}

\subsection{制御モデルの全体像}
本研究では、VPPを構成する複数の充電ステーションをエージェントとし、各エージェントが配下の複数EVの充放電出力を決定する階層型の構造を採る。図\ref{fig:fig1}のように、学習時はcriticがactorを評価しactorが方策を更新するため中央集権的である。しかし実行時には、各ステーションは（i）自ステーションに接続中EVの状態（SoC、残滞在時間、目標SoC等）と（ii）市場から与えられる指令信号のみを用いて行動し、中央集権的なcriticはこの時利用されていないため、ステーション間の高頻度な個別通信を前提としない。

\begin{figure}[H]
    \centering
    \includegraphics[width=80mm]{images/fig1.png}
    \caption{EV充放電制御モデルの全体像\\Fig. 1. Overview of the EV charging/discharging control model}
    \label{fig:fig1}
\end{figure}

\subsection{階層型MADDPGの実装}
本モデルはMADDPG\cite{Lowe2020}を基礎とするが、Sheikhらの提唱するDecomposed Multi-Agent Deep Deterministic Policy Gradient (DE-MADDPG)\cite{Sheikh2020}を参考に、局所目的達成度を評価するローカルcriticと全体目的を評価するグローバルcriticを併用する。通常のMADDPGと異なる点は、critic入力を2系統に分解し、ローカル側は局所観測のみに基づく方策評価、グローバル側は全体情報に基づく方策評価を同時に学習する点である。ここでは $x\in\{\mathrm{L}、\mathrm{G}\}$ を用いてローカル／グローバルを統一的に表す。

ローカルcriticは $\zeta^{\mathrm{L}}=(o_i、a_i)$ を入力とし、ステーション$i$の局所観測 $o_i$ と行動 $a_i=\mu_i(o_i;\theta_i)$ のみから、局所目標に対する方策の価値 $Q^{\mathrm{L}}(\zeta^{\mathrm{L}})$ を推定する。一方、グローバルcriticは $\zeta^{\mathrm{G}}=(s、a)$ を入力とし、全体状態 $s$ と joint action $a=(a_1、\dots、a_N)$ に基づいて、全体目標に対する価値 $Q^{\mathrm{G}}(\zeta^{\mathrm{G}})$ を推定する。報酬設計としては、$r^{\mathrm{L}}$ に出発時の目標SoC達成度（未達分が大きいほど低評価）を中心に与え、学習を安定化させるために充放電量に比例した整形項を併用する。$r^{\mathrm{G}}$ は指令追従を目的として、要求電力と全ステーション合計電力の偏差に基づき、許容帯域内では正の報酬、帯域外では偏差に応じたペナルティを与える。

actor更新はDE--MADDPGの基本形に従い、ローカル目的とグローバル目的から得られる更新方向を重み$w$で凸結合する：
\begin{equation}
\theta_i
\leftarrow
\theta_i
+\alpha\,\nabla_{\theta_i}\Bigl(
(1-w)\,J_i^{\mathrm{L}}
+w\,J_i^{\mathrm{G}}
\Bigr),
\qquad 0\le w\le 1.
\label{eq:mix_update}
\end{equation}
ここで $J_i^{\mathrm{L}}$ と $J_i^{\mathrm{G}}$ は、それぞれローカルcriticとグローバルcriticに基づく（標準的な）方策勾配の目的関数であり、replay buffer $\mathcal{D}$ とtarget networkを用いて学習する。


\section{ケーススタディ}
本研究は、日本の需給調整市場における二次調整力相当の参加を想定し、指令信号が5分ごとに到来する離散時間環境（1ステップ=5分）で評価する。1日を288ステップとして1エピソードを構成し、VPPは5ステーション、各10台の普通充電器（合計50台）からなるものとする。EV到着時刻はElaadNL--職場分布\cite{ElaadNLWorkplace}、到着時SoCはDESL-EPFLのlv3充電器データセット\cite{DistributedElectricalSystemsLaboratoryDESL2025}、目標SoC・滞在時間はACN（JPL）\cite{CaliforniaInstituteofTechnology2019}（電池容量情報がないためEV容量を100kWhと仮定）、ディスパッチ信号はPJM RTO Regulation Signal（2024年10月）\cite{PJMInterconnection2024}を用いた。

本ケーススタディでは、性能指標として（i）Target SoC satisfaction rate、（ii）Dispatch tracking rate を用いる。ここで(i)は、各 EV の出発時刻において SoC が目標値以上であった出発事象の割合として定義する。
(ii)は、全ステップの内VPP 集約出力が指令値の許容帯（日本の市場仕様に従い入札幅 ×0.1を許容幅とする）内であった
ステップの割合として算出する。

提案手法は 600 エピソードで学習し，バッチサイズ 1024，割引率 $\gamma = 0.975$ とした。
actor/critic の隠れ層サイズはいずれも 256 とした。
actor 更新では local/global critic の評価を重み $w = 0.3$ で凸結合して用いた。
探索は OU ノイズと $\varepsilon$-greedy を併用し，強度は学習の進行に合わせて減衰させた。

\section{結果・考察}
学習の進行に伴う性能指標の推移を図\ref{fig:fig2}に示す。学習の初期段階から指令追従性能が改善し、収束近傍では安定した追従が観察される。収束付近の方策で評価した結果、Target SoC satisfaction rate は 63.5％、Dispatch tracking rate は 99.5％ であった。これは密で低分散な報酬を与える global の勾配が支配しやすく、疎で遅延のある local は信用割当が難しく学習信号が弱くなるため、結果として global 側に偏ると考えられる。同条件の集中最適化では前述2指標とも100％を達成することを考えると、本手法は一定の成果を示したが、さらなる改善の余地があると考えられる。

\begin{figure}[H]
    \centering
    \includegraphics[width=80mm]{images/fig2.png}
    \caption{学習に伴う性能指標の推移\\Fig. 2. Performance metrics over training episodes}
    \label{fig:fig2}
\end{figure}

代表的なテストエピソードにおけるステーション出力を図\ref{fig:fig3}に示す。ステーション単位の分散実行でありながら、紫の破線により表現される集約出力が、黒の実線で示される系統運用者から与えられる指令信号値に追従する協調挙動が得られていることが確認できる。
\begin{figure}[H]
    \centering
    \includegraphics[width=80mm]{images/fig3.png}
    \caption{代表テストにおけるステーション電力\\Fig. 3. Station power in a representative test episode}
    \label{fig:fig3}
\end{figure}



\section{まとめ}
本稿では、需給調整市場におけるEV–VPPの低遅延制御を目的として、階層型MADDPGに基づく制御枠組みを提案した。ユーザ需要（出発時SoC確保）と指令追従の二つの目標を、それぞれの評価に基づく方策更新により扱うことで、分散実行下で両目的を同時に扱う構成を示した。シミュレーションでは指令追従が集中最適化に匹敵する一方、ユーザ需要満足率の向上には課題が残った。今後は、報酬設計および学習率・割引率・目的重み係数等のハイパーパラメータ調整により、両目的の達成水準の向上を図るとともに、ユーザ満足率を100％に近づけるため、ルールベースの安全層を併用したハイブリッド制御を検討する。具体的には、残時間と現在SoCから目標SoC達成が物理的に困難と判定される場合に限り、actor出力を上書きして最大充電を指令し、出発直前の未充足を回避する。さらに、EV・充電器の異質性、予測誤差、通信遅延等を含む実環境に近い条件での追加評価と実証が必要である。
\begingroup
\fontsize{8pt}{12pt}\selectfont
\renewcommand{\refname}{文献}
\bibliographystyle{unsrt}
\bibliography{references}
\endgroup

\end{document}
